_wandb:
    value:
        cli_version: 0.23.0
        e:
            lf3dvmd8nbtfhge7lbguw7uho74a7h0t:
                codePath: env\src\linear_transformer.py
                codePathLocal: src\linear_transformer.py
                cpu_count: 16
                cpu_count_logical: 24
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "4000750497792"
                        used: "2133146816512"
                email: brodiehoener@icloud.com
                executable: C:\Users\brodi\AppData\Local\Python\pythoncore-3.12-64\python.exe
                git:
                    commit: 36e2e83ac18fd4daa08ca0eeb86b96332f69b3ef
                    remote: https://github.com/bhoener/atlas.git
                gpu: NVIDIA GeForce RTX 4060 Ti
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 4352
                      memoryTotal: "17175674880"
                      name: NVIDIA GeForce RTX 4060 Ti
                      uuid: GPU-6dba68c7-bf17-a382-3cea-b36a6ebee8a9
                host: MSI
                memory:
                    total: "34192588800"
                os: Windows-11-10.0.26100-SP0
                program: E:\Other\ATLAS\env\src\linear_transformer.py
                python: CPython 3.12.10
                root: E:\Other\ATLAS\env
                startedAt: "2026-02-08T22:17:18.528734Z"
                writerId: lf3dvmd8nbtfhge7lbguw7uho74a7h0t
        m: []
        python_version: 3.12.10
        t:
            "1":
                - 1
                - 105
            "2":
                - 1
                - 105
            "3":
                - 16
            "4": 3.12.10
            "5": 0.23.0
            "8":
                - 3
            "12": 0.23.0
            "13": windows-amd64
adam_lr:
    value: 0.0003
batch_size:
    value: 2
d_model:
    value: 256
max_steps:
    value: 10000
muon_lr:
    value: 0.0003
n_heads:
    value: 8
n_layers:
    value: 8
p:
    value: 2
r:
    value: 4
seq_len:
    value: 128
vocab_size:
    value: 50304
